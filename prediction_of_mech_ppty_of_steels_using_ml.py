# -*- coding: utf-8 -*-
"""Prediction of Mech Ppty of Steels Using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xNMcXBySvcA-6TIWNuWeB8rWldVx5BfH

Multi-output Regression using Scikit-learn
Mechanical Properties of Steels

# Step 1: Downloading and unzipping the dataset
"""

# Download dataset
!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/hxpvf9gjz9-1.zip

#unzip the dataset
!unzip -q /content/hxpvf9gjz9-1.zip

"""# Step 2: Reading dataset using Pandas
Now, we are using the pandas library to read this excel file and store its contents in a dataframe named ‘df’.
Once the data is loaded into df, we display the first 5 rows of the dataframe using the head() method.

"""

'''
•	X1 STT(℃) - Solution Treatment Temperature
•	X2 ATT(℃ - Aging Treatment Temperature
•	X3 C(%) - Carbon
•	X4 Mn(%) - Manganese
•	X5 Si(%) - Silicon
•	X6 Cr(%) - Chromium
•	X7 Mo(%) - Molybdenum
•	X8 RE(%) - Rhenium
•	X9 Al(%) - Aluminum
•	X10 V(%) - Vanadium
Outputs (Dependent variables)
•	y1 Fatigue
•	y2 Tensile
•	y3 Fracture
•	y4 Hardness
'''

import pandas as pd
df = pd.read_csv('/content/Mechanical Properties of Steels/Original Data/NIMS Fatigue.csv')
#df = pd.read_excel('/content/4vnf8rh5gm-1/data.xlsx')
df.head()

"""#Step 3: Exploratory data analysis (EDA)

"""

# Finding correlation between features and plotting them
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize = (8,5))
sns.heatmap(df.corr(), cbar = False, annot = True, fmt=".1f")

"""## (ii) Plotting histograms to visualize data distribution

"""

import matplotlib.pyplot as plt

# Define the number of rows and columns you want
n_rows=4
n_cols=4

# Create the subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)
fig.set_size_inches(10, 5)
for i, column in enumerate(df.iloc[:, :-4].columns):
    sns.histplot(df[column], ax=axes[i//n_cols, i % n_cols], kde=True)
plt.tight_layout()

"""## (iii) Spliting the dataset

With random_state=42 , we get the same train and test sets across different executions, but in this time, the train and test sets are different from the previous case with random_state=0 . The train and test sets directly affect the model's performance score.
"""

# Split the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X = df.iloc[:,:-4]
y = df.iloc[:,-4:]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

"""## (iv) Finding unique values in each feature to understand the variance:"""

for col in X_train.columns:
    print(f"{col} : ", X_train[f'{col}'].unique())

"""## (v) Plot of each feature with respect to UTS(MPa)
Exploring the relationships between each feature in the dataframe and the output variables 'UTS(MPa)'.

"""

#y1 Fatigue
#y2 Tensile
#y3 Fracture
#y4 Hardness

# Create the subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)
fig.set_size_inches(10, 5)
for i, column in enumerate(df.iloc[:,:-4].columns):
    sns.regplot(x = df[column], y = df['Fatigue'],ax=axes[i//n_cols,i%n_cols], scatter_kws={"color": "green"}, line_kws={"color": "red"})
plt.tight_layout()

"""## (vi) Plot of each feature with respect to 'YTS(Mpa)',	'EL(%)'

"""

#y1 Fatigue
#y2 Tensile
#y3 Fracture
#y4 Hardness
# Create the subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)
fig.set_size_inches(10, 5)

for i, column in enumerate(df.iloc[:,:-4].columns):
    sns.regplot(x = df[column], y = df['Tensile'],ax=axes[i//n_cols,i%n_cols], scatter_kws={"color": "black" , 'cmap':'jet'}, line_kws={"color": "red"})
plt.tight_layout()

"""## (vi) Plot of each feature with respect to 'EL(%)'"""

#y1 Fatigue
#y2 Tensile
#y3 Fracture
#y4 Hardness
# Create the subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)
fig.set_size_inches(10, 5)

for i, column in enumerate(df.iloc[:,:-4].columns):
    sns.regplot(x = df[column], y = df['Fracture'],ax=axes[i//n_cols,i%n_cols], scatter_kws={"color": "blue" , 'cmap':'jet'}, line_kws={"color": "red"})
plt.tight_layout()

#y1 Fatigue
#y2 Tensile
#y3 Fracture
#y4 Hardness
# Create the subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)
fig.set_size_inches(10, 5)

for i, column in enumerate(df.iloc[:,:-4].columns):
    sns.regplot(x = df[column], y = df['Hardness'],ax=axes[i//n_cols,i%n_cols], scatter_kws={"color": "yellow" , 'cmap':'jet'}, line_kws={"color": "red"})
plt.tight_layout()

"""#Step 4: Compute summary of predictions


"""

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

def printPredictions(y_true,y_pred, count):
  print(f"Predictions: ")
  print(y_true.assign(
      Y1_pred = y_pred[:,0],
      Y2_pred = y_pred[:,1],
      Y3_pred = y_pred[:,2],
      Y4_pred = y_pred[:,3]
  ).head(count).to_markdown(index = False))

def showResults(y_true, y_pred, count = 5):
  print("R2 score: ",r2_score(y_true,y_pred))
  print("Mean squared error: ",mean_squared_error(y_true,y_pred))
  print("Mean absolute error: ",mean_absolute_error(y_true,y_pred))
  printPredictions(y_true,y_pred, count)

"""#Step 5: Splitting data into training and test set.
80% is used for training, and 20%for testing. The random_state parameter ensures reproducibility.

"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(df.iloc[:,:-4], df.iloc[:,-4:], test_size = 0.2, random_state = 42)
print(X_train.shape,X_test.shape)
print(y_train.shape, y_test.shape)

"""# Step 6: Creating multi-output regression model
The multi-output regression using Scikit-learn can be done in three ways:

"""

#(i) Linear regression
from sklearn.linear_model import LinearRegression
linear = LinearRegression()
linear.fit(X_train,y_train)
showResults(y_test,linear.predict(X_test))

# (ii) Random forest regressor
from sklearn.ensemble import RandomForestRegressor
rdf = RandomForestRegressor()
rdf.fit(X_train,y_train)
showResults(y_test,rdf.predict(X_test))

# (iii) Extra trees regressor
from sklearn.ensemble import ExtraTreesRegressor
extra_reg = ExtraTreesRegressor()
extra_reg.fit(X_train,y_train)
showResults(y_test,extra_reg.predict(X_test))

#(iv) K-neighbours regressor
from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor()
knn.fit(X_train,y_train)
showResults(y_test,knn.predict(X_test))

"""##2. Using MultiOutputRegressor()
Here we train a separate regressor for each target variable.
We implement a multi-output regression model using the Support Vector Regressor (SVR) from scikit-learn.
"""

from sklearn.multioutput import MultiOutputRegressor
from sklearn.svm import SVR

svm_multi = MultiOutputRegressor(SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1))
svm_multi.fit(X_train,y_train)
showResults(y_test,svm_multi.predict(X_test))

"""## 3. Chained Multi-output Regression : Regression Chain
In this approach we organize individual regression models into a sequence or “chain.”
Each model in the chain predicts a target label based on all available input features and the predictions of previous models in the sequence. This chaining strategy leverages both feature information and the insights gained from earlier model predictions to make multi-output predictions.

"""

# Defining the chained multioutput model
from sklearn.multioutput import RegressorChain
from sklearn.linear_model import LinearRegression

#svm_chain = RegressorChain(LinearRegression())
svm_chain = RegressorChain(SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1))
svm_chain.fit(X_train,y_train)
showResults(y_test,svm_chain.predict(X_test))

